{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice with Jupyter Notebooks and Basic Python Commands\n",
    "\n",
    "**Overview:** For this assignment, you will practice working with Jupyter notebooks, navigate documentation for Python packages and functions, and read in the data you have selected for your class project. \n",
    "\n",
    "**Directions:** Please work through the notebook, answering all the questions. \n",
    "When you are done, export to a PDF file. Upload it to Gradescope using the following naming convention for your file: \n",
    "\n",
    "DSC201_602_SP24_codingassignment4_unityID\n",
    "\n",
    "(For example, DSC201_602_SP24_codingassignment4_keporte2)\n",
    "\n",
    "As a reminder, you can export to a html file within VS Code by clicking on the three dots (\"...\") to the right of \"Outline\" in the panel at the top of your screen. Then print your html to a PDF. If you have trouble exporting in VS Code, please upload your file to Jupyter Hub (https://jhub.cos.ncsu.edu/) by clicking on the up error over the horizontal line (2 icons to the right of the blue plus button). Then you can export to a PDF by going to \"File\" then \"Save and Export Notebook As.\" \n",
    "\n",
    "**Points:** 25 points (plus 3 extra credit points)\n",
    "\n",
    "**Due:** February 14 at 11:59 PM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `NumPy` package\n",
    "\n",
    "`NumPy` is a popular Python package used primarily for numerical calculations.  It is a powerful tool that handles large sets of numbers and mathematical operations efficiently. It's widely used in data science to process and analyze data. `NumPy`'s ability to perform complex calculations quickly makes it a foundational tool for other data science libraries. \n",
    "\n",
    "Uncomment and run the command below to install `NumPy`. Note you will need to retstart your kernel after you install it. In VS Code and on Jupyter Hub, there is a restart button in the top panel. You can comment out the installation command after you have successfully installed the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import `numpy`. We shorten the imported name to np for better readability of code using NumPy. This is a widely adopted convention that makes your code more readable for everyone working on it. \n",
    "\n",
    "Then, `help(np)` brings up documentation. Go to the listed link of numpy.org and after opening it, click on \"Documentation.\" Find the answers to the following questions: \n",
    "\n",
    "**(1) Similar to how R's `dplyr` has somewhat different versions of the data structures in base R, Python's `numpy` has somewhat different versions of the data structures in standard Python. For example, an \"array\" is a central data structure in `numpy` which can be compared to a \"list\" in standard Python. What is at least one difference between a \"list\" in `numpy` and an \"array\" in standard Python? [Enter your answer below, 1 POINT]**\n",
    "\n",
    "Standard Python Lists: They are heterogeneous, meaning they can contain elements of different data types within the same list (e.g., integers, strings, objects).\n",
    "NumPy Arrays: These are homogeneous, meaning all elements in a NumPy array must be of the same data type. This constraint allows NumPy to efficiently perform vectorized operations, utilizing contiguous memory allocation and optimized C and Fortran libraries for fast numerical computations.\n",
    "\n",
    "**(2) In `numpy`, what is the relationship between a vector and array? What is relationship between a matrix and array? [Enter your answer below, 1 POINT]**\n",
    "\n",
    "\n",
    "A vector in NumPy is essentially a 1D array. It is a single row of elements or a single column of elements â€“ essentially a list of numbers.\n",
    "\n",
    "A matrix in NumPy is a 2D array. It is a rectangular grid of elements arranged in rows and columns. Each row and each row is a vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# uncomment to test, but then recomment so you don't have this long output in your final document\n",
    "# help(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3) In the following code chunk, write a line a code to accomplish each task described in the comments. Refer to documentation at numpy.org to guide you; it provides lots of examples that you can copy and modify. [6 POINTS TOTAL, 1 POINT EACH]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "1\n",
      "10\n",
      "5.5\n",
      "[[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]]\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Create a one-dimensional NumPy array of numbers from 1 to 10.\n",
    "my_array = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "#my_array = np.arange(1, 11) # This also works. \n",
    "print(my_array)\n",
    "\n",
    "# TODO: Print the first element of the array you just created.\n",
    "print(my_array[0])\n",
    "\n",
    "# TODO: Find the length of the array.\n",
    "print(len(my_array))\n",
    "\n",
    "# TODO: Find and print the mean of your array\n",
    "print(np.mean(my_array))\n",
    "\n",
    "# TODO: Reshape your array into a matrix with 2 columns and 5 rows, assign this to a new object and print it (2 lines of code)\n",
    "my_matrix = my_array.reshape(5,2)\n",
    "print(my_matrix)\n",
    "\n",
    "# TODO: Find and print the sum of the first column of your matrix\n",
    "sum_first_column = np.sum(my_matrix[:, 0])\n",
    "print(sum_first_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4) A valuable way to expand your coding skills is to run example code - from documentation, tutorials, other coders - and see what it accomplishes. Using the documentation at numpy.org, create a code chunk and test out three NumPy commands that either operate on the array or matix that you created above. After running the commands, use comments to explain what the command does and anything you think is important to remember about how the command is used, or something you do not understand. (3 POINTS FOR COMPLETION)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_matrix.sum(axis=0)\n",
    "my_array.reshape(10,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with `pandas`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While NumPy is valuable for computations on arrays and matrices, we will work more with Pandas (which is built on top of NumPy), which is valuable for more complex data manipulation tasks on structured data. You should have already installed pandas when running Python-Notebook1. If not, uncomment and run the first line in the chunk below. In either case, run the import command below to load the Pandas library into your environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/6y_6pk4x2lscbcf2_4ybm7wr0000gn/T/ipykernel_11281/1009365360.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas\n",
    "\n",
    "# Import the pandas library and assign the alias \"pd\"\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing documentation\n",
    "\n",
    "For `pandas` documentation, check out: https://pandas.pydata.org/docs/ (which is much more user friendly than what you get with help(pd).) Review the \"package overview\" and answer the following questions:\n",
    "\n",
    "**(5) What are at least two things you can do with the `pandas` package that accomplish similar (or identical) tasks you learned with `dplyr` in R? [1 POINT]**\n",
    "\n",
    "1. **Filtering Data**:\n",
    "   - In `dplyr`, you use `filter()` to select rows based on a condition. For example: `filter(df, condition)`.\n",
    "   - In `pandas`, you can achieve this with boolean indexing or the `.query()` method. For example: `df[df['column'] condition]` or `df.query(\"column condition\")`.\n",
    "\n",
    "2. **Selecting Columns**:\n",
    "   - `dplyr` uses `select()` to choose specific columns from a dataframe. For example: `select(df, column1, column2)`.\n",
    "   - In `pandas`, you can select columns by passing a list of column names. For example: `df[['column1', 'column2']]`.\n",
    "\n",
    "3. **Creating or Transforming Columns**:\n",
    "   - In `dplyr`, `mutate()` is used to create new columns or modify existing ones. For example: `mutate(df, new_column = existing_column + 10)`.\n",
    "   - In `pandas`, you can directly create or modify columns using assignment. For example: `df['new_column'] = df['existing_column'] + 10`.\n",
    "\n",
    "4. **Grouping and Summarizing Data**:\n",
    "   - `dplyr` uses `group_by()` in combination with `summarise()` (or `summarize()`) to group data and then apply summary functions. For example: `df %>% group_by(group_column) %>% summarise(mean_value = mean(value_column))`.\n",
    "   - In `pandas`, this is achieved using `.groupby()` followed by aggregation methods like `.mean()`. For example: `df.groupby('group_column')['value_column'].mean()`.\n",
    "\n",
    "5. **Sorting Data**:\n",
    "   - `dplyr` provides `arrange()` for sorting data frames. For example: `arrange(df, column)`.\n",
    "   - In `pandas`, the equivalent is the `.sort_values()` method. For example: `df.sort_values(by='column')`.\n",
    "\n",
    "\n",
    "**(6) What are the two main data structures in `pandas` and what are the parallel data structures in base R or `dplyr`? [1 POINT]**\n",
    "\n",
    "In `pandas`, the two main data structures are:\n",
    "\n",
    "1. **DataFrame**: \n",
    "   - This is a two-dimensional, size-mutable, and potentially heterogeneous tabular data structure with labeled axes (rows and columns). It's akin to a spreadsheet or SQL table.\n",
    "   - **Parallel in R**: The equivalent in base R would be a `data.frame`. In `dplyr`, a `DataFrame` is conceptually similar to a `tbl_df` (or tibble), which is a modern reimagining of the data frame.\n",
    "\n",
    "2. **Series**: \n",
    "   - A Series is a one-dimensional array-like object containing a sequence of values and an associated array of data labels, called its index. It's similar to a single column in a spreadsheet.\n",
    "   - **Parallel in R**: The closest equivalent to a `Series` in R is a `vector`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(7) Within this Pandas website, go to https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_r.html#quick-reference. Use this \"quick reference\" to complete the following. [4 POINTS TOTAL, 1 POINT EACH]**\n",
    "\n",
    "Note: The functions are preceded by `pandas` and a \".\" or more frequently and preferably, the alias you used when you imported it (pd). So \"pd.functionname.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ColA  ColB   ColC\n",
      "0     1   0.1   True\n",
      "1     2   0.2  False\n",
      "2     3   0.3   True\n",
      "Dimensions of DataFrame: (3, 3)\n",
      "Summary of all columns:\n",
      "         ColA  ColB  ColC\n",
      "count    3.0  3.00     3\n",
      "unique   NaN   NaN     2\n",
      "top      NaN   NaN  True\n",
      "freq     NaN   NaN     2\n",
      "mean     2.0  0.20   NaN\n",
      "std      1.0  0.10   NaN\n",
      "min      1.0  0.10   NaN\n",
      "25%      1.5  0.15   NaN\n",
      "50%      2.0  0.20   NaN\n",
      "75%      2.5  0.25   NaN\n",
      "max      3.0  0.30   NaN\n",
      "Reduced DataFrame:\n",
      "    ColA  ColB\n",
      "0     1   0.1\n",
      "1     2   0.2\n",
      "2     3   0.3\n",
      "Number of True values in 'ColC': 2\n"
     ]
    }
   ],
   "source": [
    "# First, I will create a toy data frame for you. \n",
    "df = pd.DataFrame({\n",
    "    'ColA': [1, 2, 3],\n",
    "    'ColB': [0.1, 0.2, 0.3],\n",
    "    'ColC': [True, False, True]\n",
    "})\n",
    "print(df)\n",
    "\n",
    "# TODO: Find the dimensions of the data frame df\n",
    "df_dimensions = df.shape\n",
    "print(\"Dimensions of DataFrame:\", df_dimensions)\n",
    "\n",
    "# TODO: Summarize all columns in the data frame df (Hint: for this one the () will be empty and you need to include \"print()\")\n",
    "summary = df.describe(include='all')\n",
    "print(\"Summary of all columns:\\n\", summary)\n",
    "\n",
    "# TODO: Create a reduced data frame (a new object) that is just the first two columns\n",
    "reduced_df = df[['ColA', 'ColB']]\n",
    "print(\"Reduced DataFrame:\\n\", reduced_df)\n",
    "\n",
    "# TODO: Run another command of your choice and add a comment describing what it does. Be sure to print the results. \n",
    "# Counting the number of observations with ColC equal to TRUE\n",
    "true_count = df['ColC'].sum()\n",
    "print(\"Number of True values in 'ColC':\", true_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For specific functions within a package, it can be quick and helpfl to pull up documentation from inside your Jupyter notebook. Using the example of `read_csv`:\n",
    " \n",
    " - help(pd.read_csv)\n",
    "\n",
    " This is a standard Python function that invokes the built-in help system. When you use help(pd.read_csv), it displays the documentation for the read_csv function in a more detailed, text-based format. This command is versatile and can be used in any Python environment, including a standard Python shell, scripts, and Jupyter notebooks. The output is typically displayed in the same area where the command was executed.\n",
    "\n",
    "- pd.read_csv?\n",
    "\n",
    "  This syntax is specific to Jupyter notebooks. \n",
    "  When you use pd.read_csv? in a Jupyter notebook, it displays the documentation in a separate pane or window at the bottom of the notebook interface. This pane can be resized, scrolled, or closed. The documentation displayed is generally more concise and is formatted for quick readability, focusing on the most essential aspects of the function.\n",
    "  This method is more interactive and user-friendly, especially in a Jupyter notebook environment, but it is not available in standard Python shells or scripts.\n",
    "\n",
    "**(8) Give these a try below, and provide 2 observations for each: [Enter responses below within this markdown chunk, 4 POINTS]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment the line below to test. \n",
    "\n",
    "# help(pd.read_csv)\n",
    "\n",
    " \n",
    "# TODO: Uncomment the line below to test. \n",
    "\n",
    "#pd.read_csv?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(9) Now, let's use `read_csv` to read in the datasests you have selected for your project. I am assuming you have .csv files. If not, then write a .csv file in R so that you can read it in here. After reading in your file, carry out the tasks described in the comments. [4 POINTS TOTAL, 1 POINT EACH]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of DataFrame: (4435, 26)\n",
      "Summary of all columns:\n",
      "                OPEID               name      city state   region  median_debt  \\\n",
      "count   4.435000e+03               4435      4435  4435     4435  4435.000000   \n",
      "unique           NaN               4357      1943    54        7          NaN   \n",
      "top              NaN  Cortiva Institute  New York    CA  Midwest          NaN   \n",
      "freq             NaN                  6        51   423     1074          NaN   \n",
      "mean    1.492464e+06                NaN       NaN   NaN      NaN    11.195790   \n",
      "std     1.976276e+06                NaN       NaN   NaN      NaN     5.319178   \n",
      "min     1.002000e+05                NaN       NaN   NaN      NaN     1.932000   \n",
      "25%     2.822000e+05                NaN       NaN   NaN      NaN     6.863000   \n",
      "50%     7.669000e+05                NaN       NaN   NaN      NaN     9.500000   \n",
      "75%     2.362002e+06                NaN       NaN   NaN      NaN    15.000000   \n",
      "max     7.209887e+07                NaN       NaN   NaN      NaN    33.470000   \n",
      "\n",
      "        default_rate highest_degree           ownership  locale  ...  \\\n",
      "count    4435.000000           4435                4435    4435  ...   \n",
      "unique           NaN              4                   3       5  ...   \n",
      "top              NaN       Graduate  Prviate for-profit  Suburb  ...   \n",
      "freq             NaN           1464                1684    1311  ...   \n",
      "mean        9.060090            NaN                 NaN     NaN  ...   \n",
      "std         6.144554            NaN                 NaN     NaN  ...   \n",
      "min         0.000000            NaN                 NaN     NaN  ...   \n",
      "25%         4.400000            NaN                 NaN     NaN  ...   \n",
      "50%         8.200000            NaN                 NaN     NaN  ...   \n",
      "75%        12.300000            NaN                 NaN     NaN  ...   \n",
      "max        57.100000            NaN                 NaN     NaN  ...   \n",
      "\n",
      "           avg_cost  net_tuition  ed_spending_per_student avg_faculty_salary  \\\n",
      "count   4435.000000  4435.000000              4435.000000        3077.000000   \n",
      "unique          NaN          NaN                      NaN                NaN   \n",
      "top             NaN          NaN                      NaN                NaN   \n",
      "freq            NaN          NaN                      NaN                NaN   \n",
      "mean      27.102880    10.836639                 7.760832           7.266518   \n",
      "std       14.988075     7.506410                 6.881391           2.528365   \n",
      "min        4.760000     0.000000                 0.000000           0.897000   \n",
      "25%       16.452500     5.439500                 4.126000           5.610000   \n",
      "50%       22.945000     9.912000                 6.352000           6.958000   \n",
      "75%       32.032500    14.218000                 9.342000           8.573000   \n",
      "max      120.377000    66.442000               139.766000          21.143000   \n",
      "\n",
      "           pct_PELL  pct_fed_loan    grad_rate  pct_firstgen  med_fam_income  \\\n",
      "count   4435.000000   4435.000000  4435.000000   4088.000000     4399.000000   \n",
      "unique          NaN           NaN          NaN           NaN             NaN   \n",
      "top             NaN           NaN          NaN           NaN             NaN   \n",
      "freq            NaN           NaN          NaN           NaN             NaN   \n",
      "mean      45.555540     49.069461    54.945651     43.357756       31.791930   \n",
      "std       20.309775     24.542281    22.051351     12.931312       20.811117   \n",
      "min        0.000000      0.000000     0.000000      8.866995        0.000000   \n",
      "25%       29.830000     30.925000    37.310000     35.006281       17.827750   \n",
      "50%       42.500000     52.540000    56.400000     45.102178       24.670000   \n",
      "75%       60.380000     67.680000    71.915000     52.599727       39.516500   \n",
      "max      100.000000    100.000000   100.000000     85.906040      179.864000   \n",
      "\n",
      "        med_alum_earnings  \n",
      "count         3912.000000  \n",
      "unique                NaN  \n",
      "top                   NaN  \n",
      "freq                  NaN  \n",
      "mean            40.007157  \n",
      "std             14.486256  \n",
      "min             10.939000  \n",
      "25%             29.720250  \n",
      "50%             38.056000  \n",
      "75%             47.381250  \n",
      "max            132.969000  \n",
      "\n",
      "[11 rows x 26 columns]\n",
      "                                  name state  admit_rate  SAT_avg\n",
      "0             Alabama A & M University    AL       89.65    959.0\n",
      "1  University of Alabama at Birmingham    AL       80.60   1245.0\n",
      "2                   Amridge University    AL         NaN      NaN\n",
      "3  University of Alabama in Huntsville    AL       77.11   1300.0\n",
      "4             Alabama State University    AL       98.88    938.0\n",
      "5            The University of Alabama    AL       80.39   1262.0\n",
      "6    Central Alabama Community College    AL         NaN      NaN\n",
      "7      Auburn University at Montgomery    AL       95.55   1061.0\n",
      "8                    Auburn University    AL       85.07   1302.0\n",
      "9          Birmingham-Southern College    AL       60.45   1202.0\n"
     ]
    }
   ],
   "source": [
    "# Read in the file by editing the example below\n",
    "projectData = pd.read_csv('data/colleges.csv')\n",
    "\n",
    "# TODO: Display the dimensions of your data frame projectData\n",
    "projectData_dimensions = projectData.shape\n",
    "print(\"Dimensions of DataFrame:\", projectData_dimensions)\n",
    "\n",
    "# TODO: Summarize (describe) the columns in projectData\n",
    "summaryProject = projectData.describe(include='all')\n",
    "print(\"Summary of all columns:\\n\", summaryProject)\n",
    "\n",
    "# TODO: Display the first 10 rows of ProjectData and at the same time, 5 columns of your choice\n",
    "selectedCols = ['name','state','admit_rate','SAT_avg']\n",
    "print(projectData[selectedCols].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credit:**\n",
    "\n",
    "**(1) Create a new object to hold a modified version of `projectData`. The modified version should (a) rename one variable, (b) drop one variable and (c) create one new variable that is a computation based on another variable in your data (e.g., it sums to variables together or it multiplies a variable by a number, etc. ). [1 POINT]**\n",
    "\n",
    "**(2) Describe what you would do to check that you modifications worked. How would you make it easy to do the checks (rather than viewing lots of information you don't need). [1 POINT]**\n",
    "\n",
    "**(3) Write code to implement your check [1 POINT]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     OPEID  admit_rate  collegeID  admit_rate_01\n",
      "0   100200       89.65     100200         0.8965\n",
      "1   105200       80.60     105200         0.8060\n",
      "2  2503400         NaN    2503400            NaN\n",
      "3   105500       77.11     105500         0.7711\n",
      "4   100500       98.88     100500         0.9888\n",
      "5   105100       80.39     105100         0.8039\n",
      "6   100700         NaN     100700            NaN\n",
      "7   831000       95.55     831000         0.9555\n",
      "8   100900       85.07     100900         0.8507\n",
      "9   101200       60.45     101200         0.6045\n",
      "['collegeID', 'name', 'city', 'state', 'region', 'median_debt', 'default_rate', 'highest_degree', 'ownership', 'locale', 'hbcu', 'SAT_avg', 'online_only', 'enrollment', 'net_price', 'avg_cost', 'net_tuition', 'ed_spending_per_student', 'avg_faculty_salary', 'pct_PELL', 'pct_fed_loan', 'grad_rate', 'pct_firstgen', 'med_fam_income', 'med_alum_earnings', 'admit_rate_01']\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "projectData_mod = projectData.copy()\n",
    "\n",
    "# 1. Rename a variable/column\n",
    "projectData_mod = projectData_mod.rename(columns={'OPEID': 'collegeID'})\n",
    "\n",
    "# 2. Create a new variable/column based on a calculation of an existing column\n",
    "projectData_mod['admit_rate_01'] = projectData_mod['admit_rate'] /100\n",
    "\n",
    "# 3. Drop a column\n",
    "projectData_mod = projectData_mod.drop('admit_rate', axis=1)\n",
    "\n",
    "# Checking with a print of columns involved\n",
    "selCol_orig = projectData[['OPEID','admit_rate']]\n",
    "selCol_mod = projectData_mod[['collegeID','admit_rate_01']]\n",
    "checkdf = pd.concat([selCol_orig,selCol_mod],axis=1)\n",
    "print(checkdf.head(10))\n",
    "\n",
    "# Also making sure OPEID and admit_rate are gone\n",
    "print(list(projectData_mod.columns))\n",
    "\n",
    "# Better yet, so I don't have to trust my reading of all the col names:\n",
    "# Returns TRUE if the column name listed is in the column names, FALSE otherwise\n",
    "print('admit_rate' in projectData_mod.columns)\n",
    "print('OPEID' in projectData_mod.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroRPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
